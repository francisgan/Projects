import torch
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import torchvision
from typing import Optional, Union
import pickle
import model_util
import configs
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
epochs = configs.T_epoch
learning_rate= configs.T_learning_rate
train_size = configs.T_training_size_scale

with open ('feature', 'rb') as fp:
    data_feature = pickle.load(fp)
fp.close()

with open ('target', 'rb') as fp:
    data_target = pickle.load(fp)
fp.close()
cap = int(len(data_feature)*train_size - 15000*train_size)
train_set = data_feature[:cap]
train_target = data_target[:cap]
valid_set = data_feature[-15000:-10000]
valid_target = data_target[-15000:-10000]
test_set = data_feature[-10000:]
test_target = data_target[-10000:]

class CustomDataset(Dataset):
    def __init__(self, features, targets):
        self.features = features
        self.targets = targets

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = torch.tensor(self.features[idx], dtype=torch.float32)
        target = torch.tensor(self.targets[idx], dtype=torch.float32)
        return feature, target
    
batch_size = configs.T_batch_size
train_dataset = CustomDataset(train_set, train_target)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

valid_dataset = CustomDataset(valid_set, valid_target)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)

test_dataset = CustomDataset(test_set, test_target)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

input_dim = 52  # Number of features per data point
d_model = 256  # Size of the Transformer embeddings
num_classes = 25  # Number of output classes
num_heads = 8  # Number of heads in the multi-head attention mechanism
num_layers = 3  # Number of Transformer layers
max_seq_len = 300  # Maximum sequence length

model = model_util.TransformerClassifier(input_dim, d_model, num_classes, num_heads, num_layers, max_seq_len).to(device)
criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for i in range(epochs):
    model.train()
    running_loss = []

    pbar = tqdm(total=len(train_loader))
    for batch_idx, batch in enumerate(train_loader):
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()  
        outputs = model(inputs)  
        loss = criterion(outputs, labels) 
        loss.backward()  
        optimizer.step() 

        running_loss.append(loss.item())
        pbar.update(1)
    pbar.close()

    model.eval()
    v_ave_loss=[]
    v_ave_acc=[]
    pbar = tqdm(total=len(valid_loader))
    for batch_idx, batch in enumerate(valid_loader):
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)
        pred = model(inputs)
        vloss =  criterion(pred, labels)

        labels = torch.max(labels, 1)[1]
        _, pred_indices = torch.max(pred, dim=1)
        labels = labels.squeeze()
        correct = (pred_indices == labels).sum().item()
        v_acc = correct / pred.size(0)
        
        v_ave_loss.append(vloss.item())
        v_ave_acc.append(v_acc)

        pbar.update(1)
    pbar.close()
    
    vloss = np.mean(v_ave_loss)
    v_acc = np.mean(v_ave_acc)
    model.train()
    print("Finish epoch {}, loss: {}, vloss: {}, vAcc: {}".format(i, np.mean(running_loss), vloss, v_acc))

model.eval()
t_ave_acc=[]
pbar = tqdm(total=len(test_loader))
for batch_idx, batch in enumerate(test_loader):
    inputs, labels = batch
    inputs, labels = inputs.to(device), labels.to(device)
    pred = model(inputs)

    labels = torch.max(labels, 1)[1]
    _, pred_indices = torch.max(pred, dim=1)
    labels = labels.squeeze()
    correct = (pred_indices == labels).sum().item()
    t_acc = correct / pred.size(0)
        
    t_ave_acc.append(t_acc)

    pbar.update(1)
pbar.close()

print("accruracy: ", np.mean(t_ave_acc))

torch.save(model.state_dict(), "./models/transformer_.pth")