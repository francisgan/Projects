# Music Generation with LSTM Networks

This report investigates the generation of music using a character-level Long Short-Term Memory (LSTM) network. Our approach entails training on a dataset of tunes expressed in ABC notation with the goal of capturing the subtleties of musical structure for autonomous new composition generation.

## Approach

We leveraged deep learning techniques, specifically LSTM models, to minimize cross-entropy loss and achieve baseline performance. Our experiments tested various conditions to observe the influence of temperature adjustments on the creativity and determinism of the output.

## Experiments and Findings

The study experimented with different network configurations, altering neuron counts and incorporating dropout to assess impacts on the model's effectiveness and the diversity of the output. Our findings underscore the LSTM network's ability to learn and mimic musical patterns, which sheds light on the possibilities for AI-assisted music composition.

### Key Findings

- Tuning the hyper-parameter led to an optimal configuration of 250 hidden neurons and a temperature of 1 in the softmax prediction.
- The LSTM model generally outperformed the RNN model, indicating better robustness with a test loss around 1.4 compared to the RNN's test loss of 2.0.

## Conclusion

The results of this study highlight the critical role of hyperparameters in striking a balance between innovation and coherence in the music generated by AI. The potential for AI to assist in music composition is significant, as evidenced by the LSTM network's capacity to replicate complex musical patterns.
