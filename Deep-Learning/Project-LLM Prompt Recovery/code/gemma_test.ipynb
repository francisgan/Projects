{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GLw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>combined_text_inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hollis Price (born October 29, 1979) is an Ame...</td>\n",
       "      <td>Convert the text into a recipe, where each ins...</td>\n",
       "      <td>Hollis Price's Basketball DreamIngredients:- 1...</td>\n",
       "      <td>Original text: Hollis Price (born October 29, ...</td>\n",
       "      <td>Original text: Hollis Price (born October 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grangewoood Independent School is an independe...</td>\n",
       "      <td>Transform the text into a cryptic crossword pu...</td>\n",
       "      <td>Across1. A school situated in a London borough...</td>\n",
       "      <td>Original text: Grangewoood Independent School ...</td>\n",
       "      <td>Original text: Grangewoood Independent School ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tap... tap... tap. I could hear him hiding som...</td>\n",
       "      <td>Translate the essence of this text into a horr...</td>\n",
       "      <td>A tap, tap, tap echoed through the walls, send...</td>\n",
       "      <td>Original text: Tap... tap... tap. I could hear...</td>\n",
       "      <td>Original text: Tap... tap... tap. I could hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man smiled as he saw a package underneath hi...</td>\n",
       "      <td>Write like Oscar Wilde: Adopt Wilde's witty an...</td>\n",
       "      <td>With a groan, he hoisted the box, an embodimen...</td>\n",
       "      <td>Original text: A man smiled as he saw a packag...</td>\n",
       "      <td>Original text: A man smiled as he saw a packag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After weeks of making phone calls we finally f...</td>\n",
       "      <td>Rewrite this text in the style of a time trave...</td>\n",
       "      <td>Weeks of frantic phone calls yielded a celesti...</td>\n",
       "      <td>Original text: After weeks of making phone cal...</td>\n",
       "      <td>Original text: After weeks of making phone cal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Hollis Price (born October 29, 1979) is an Ame...   \n",
       "1  Grangewoood Independent School is an independe...   \n",
       "2  Tap... tap... tap. I could hear him hiding som...   \n",
       "3  A man smiled as he saw a package underneath hi...   \n",
       "4  After weeks of making phone calls we finally f...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Convert the text into a recipe, where each ins...   \n",
       "1  Transform the text into a cryptic crossword pu...   \n",
       "2  Translate the essence of this text into a horr...   \n",
       "3  Write like Oscar Wilde: Adopt Wilde's witty an...   \n",
       "4  Rewrite this text in the style of a time trave...   \n",
       "\n",
       "                                      rewritten_text  \\\n",
       "0  Hollis Price's Basketball DreamIngredients:- 1...   \n",
       "1  Across1. A school situated in a London borough...   \n",
       "2  A tap, tap, tap echoed through the walls, send...   \n",
       "3  With a groan, he hoisted the box, an embodimen...   \n",
       "4  Weeks of frantic phone calls yielded a celesti...   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Original text: Hollis Price (born October 29, ...   \n",
       "1  Original text: Grangewoood Independent School ...   \n",
       "2  Original text: Tap... tap... tap. I could hear...   \n",
       "3  Original text: A man smiled as he saw a packag...   \n",
       "4  Original text: After weeks of making phone cal...   \n",
       "\n",
       "                                   combined_text_inf  \n",
       "0  Original text: Hollis Price (born October 29, ...  \n",
       "1  Original text: Grangewoood Independent School ...  \n",
       "2  Original text: Tap... tap... tap. I could hear...  \n",
       "3  Original text: A man smiled as he saw a packag...  \n",
       "4  Original text: After weeks of making phone cal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "#    df['combined_text'] = \"<|original|> Original text: \"+ df['original_text'] + \" <|rewritten|> Rewritten text:\" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \" + df['prompt']\n",
    "#    df['combined_text_inf'] = \"<|original|> Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \"\n",
    "#    return df\n",
    "\n",
    "def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "    df['combined_text'] = \"Original text: \"+ df['original_text'] + \" <|endoftext|> Rewritten text:\" + df['rewritten_text'] + \"<|endoftext|> The prompt used to generate rewritten text given original text is: \" + df['prompt']\n",
    "    df['combined_text_inf'] = \"Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|endoftext|> The prompt used to generate rewritten text given original text is: \"\n",
    "    return df\n",
    "\n",
    "#def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "#    df['combined_text'] = \"Original text: \"+ df['original_text'] + \" <|endoftext|> Rewritten text:\" + df['rewritten_text'] + \"<|endoftext|> A prompt was used to revise rewritten original text into rewritten text. The prompt is: \" + df['prompt']\n",
    "#    df['combined_text_inf'] = \"Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|endoftext|> A prompt was used to revise rewritten original text into rewritten text. The prompt is: \"\n",
    "#    return df\n",
    "\n",
    "# Apply the function to each row in the 'rewritten_text' column\n",
    "df = add_special_tokens(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "scs_model = SentenceTransformer(\"sentence-t5-base\")\n",
    "\n",
    "def  batch_cosine_similarity(x1, x2):\n",
    "\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "def sharpened_cosine_similarity_batch(scs_model, output_texts, target_texts, sharpen_factor=3):\n",
    "    # Assuming scs_model.encode() returns PyTorch tensors\n",
    "    target_embeddings = scs_model.encode(target_texts, convert_to_tensor=True)\n",
    "    output_embeddings = scs_model.encode(output_texts, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate batch cosine similarities using the previously defined batch_cosine_similarity function\n",
    "    cos_sims = batch_cosine_similarity(target_embeddings, output_embeddings)\n",
    "    \n",
    "    # Apply the sharpening factor to each similarity score and create a list of tensors\n",
    "    sharpened_scores = [cos_sims[i][i].unsqueeze(0) ** sharpen_factor for i in range(cos_sims.size(0))]\n",
    "    \n",
    "    return sharpened_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('model/gemma/final/')\n",
    "#model = AutoModelForCausalLM.from_pretrained('model/gemma/1v2/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8673, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/573 [00:00<?, ?it/s]C:\\Users\\GLw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 573/573 [06:33<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "scs = []\n",
    "CLEAN=0\n",
    "for i in tqdm(range(8100,8673)):#input_text = \"I get it now. You have to give to get. Where's did I put the cereal? You ca n't have life without death. Or love without hate. I think this milk is sour. Maybe it's fine. Maybe I'm fine, just the way I am.   I have to shower before I go. Maybe the only reason everything is so shit is because I expect it to be. A positive attitude might be the only thing I need. Shit, I'm going to be late.   Learn to drive, asshole! I need to relax, like in that dream, some real Zen stuff. Love and equality and all that. Fuck, I spilled my coffee. Today is going to be hell.<|endoftext|> The cosmic tapestry of existence unraveled before me, illuminating the interconnectedness of all phenomena. I perceive the enigmatic axiom woven through the threads of life and death, love and hate. It is an axiom inscribed in the cosmic consciousness, a testament to the duality of existence.Through the prism of perception, I discern the paradoxical dance of giving and receiving. To acquire the sustenance necessary for survival, one must offer a reciprocal gift, a harmonious exchange of energy. The cosmic rhythm dictates that to emerge unscathed, one must embrace the yin and yang of life, death, and all that encompasses them.The sourness of the milk serves as a reminder of the impermanence of all temporal endeavors. Yet, within the abyss of doubt, I find solace in the recognition of my own intrinsic wholeness. Perhaps, the paradox of existence is reconciled through a profound sense of self-<|endoftext|>\"  # This is your input text\n",
    "    input_text = df[\"combined_text_inf\"][i]\n",
    "    prompt = df[\"prompt\"][i]\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    # Adjust the max_length parameter as needed\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=len(input_ids[0])+30,  # Adjust the max length of the output text if needed\n",
    "        temperature=0.1,  # Temperature controls the randomness of the output\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Nucleus sampling\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1  # Number of output sequences to generate\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "    #print(\"Generated text:\", generated_text)\n",
    "    pred_prompt = generated_text.split(\"The prompt used to generate rewritten text given orignal text is:\")[-1].split('\"')[0]\n",
    "    if len(pred_prompt.split('.')[0])<=5:\n",
    "        pred_prompt = pred_prompt.split('.')[1]\n",
    "    if pred_prompt[:50].find(\"<|endoftext|>\")!=-1 or pred_prompt[:50].find(\"text:\")!=-1:\n",
    "        pred_prompt = \"rewrite this text\"\n",
    "    a=[pred_prompt]\n",
    "    b=[prompt]\n",
    "    score = sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)\n",
    "    scs.append(score[0].item())\n",
    "    CLEAN+=1\n",
    "    if CLEAN%100==0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.538576277086664"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GLw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m pred_prompt \u001b[38;5;241m=\u001b[39m generated_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe prompt used to generate rewritten text given original text is:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     pred_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mpred_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     28\u001b[0m     pred_prompt \u001b[38;5;241m=\u001b[39m pred_prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m pred_prompt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i=8009\n",
    "\n",
    "input_text = df[\"combined_text_inf\"][i]\n",
    "prompt = df[\"prompt\"][i]\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    # Adjust the max_length parameter as needed\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=len(input_ids[0])+40,  # Adjust the max length of the output text if needed\n",
    "    temperature=1.0,  # Temperature controls the randomness of the output\n",
    "    top_k=50,  # Top-k sampling\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1  # Number of output sequences to generate\n",
    ")\n",
    "\n",
    "    # Decode the output\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "    #print(\"Generated text:\", generated_text)\n",
    "pred_prompt = generated_text.split(\"The prompt used to generate rewritten text given original text is:\")[-1].split('.\"')[0].split('\"')[0]\n",
    "if len(pred_prompt.split('.')[0])<=5:\n",
    "    pred_prompt = pred_prompt.split('.')[1]\n",
    "if len(pred_prompt.split('.'))>2:\n",
    "    pred_prompt = pred_prompt.split('.')[0] + pred_prompt.split('.')[1]\n",
    "if pred_prompt[:50].find(\"<|endoftext|>\")!=-1 or pred_prompt[:100].find(\"text:\")!=-1:\n",
    "    pred_prompt = \"rewrite this text\"\n",
    "\n",
    "\n",
    "a=[pred_prompt]\n",
    "b=[prompt]\n",
    "score = sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)\n",
    "print(score)\n",
    "print(pred_prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Original text: I woke up with a splitting headache.\\'I\\'m never drinking that much again. Worst hangover ever.\\' I look around but do n\\'t recognize where I am. I look down and I am in a very expensive looking tux and with all sorts of jewelry.\\'Where did I get this stuff?\\' In between Rolexes on my wrist I notice a FitBit with an ubserd number of steps on it. I try to process everything going on when I start to hear sirens.\\'Thank god. Maybe some of them will have answers.\\' I look around the room I\\'m in to try and get a sense of where I am. I round the corner and am shocked with what I see.\\'Those sirens are n\\'t coming to help me. I need to leave... now!\\' <|endoftext|> Rewritten text:  Veni, Vidi, HungoverMy dear reader, I awoke this morning with a splitting headache that could rival the roar of a lion, and a hangover so severe it made me question the very meaning of existence. With my head pounding and my stomach churning, I surveyed the scene before me, my memory as hazy as a Roman legion\\'s march through enemy territory.Look around, my friend, and you might be mistaken for a wealthy Roman noble. Tuxedos and jewels adorn me like a gilded sarcophagus. But alas, my dear reader, this luxurious attire did not come with an invitation to the orgies of the Caesars. Instead, it was a testament to my questionable judgment and insatiable appetite for potent libations.In between Rolexes on my wrist, I spotted a FitBit boasting an impressive number of steps traversed. I tried to process the surrealism of my situation, but my brain was as mushy as a<|endoftext|> The prompt used to generate rewritten text given original text is: <em>\"This script converts words into simple sentences using NLTK\\'s pretrained model (BERT-base-uncased).\"</em><eos>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2016 Presidential Race Script\"\\n\"Original text: He was dead. He went down. He had massive trauma to his face. If he was n\\'t dead, he would be'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text.split(\"The prompt used to generate rewritten text given orignal text is:\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> Rewritten text:This is a very inspiring article I have a question about your rmse'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = \"<|endoftext|> Rewritten text:This is a very inspiring article I have a question about your rmse\"\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8860], device='cuda:0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\"Rewrite the text with a humorous and ancient Roman theme.\"]\n",
    "b=[prompt]\n",
    "sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1) if 1==2 else print(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
