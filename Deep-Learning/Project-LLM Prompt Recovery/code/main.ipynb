{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5086e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 04:07:28.604236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/z4xia/.local/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer,GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8face0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()  # Releases all unused cached memory from PyTorch\n",
    "# print(torch.cuda.memory_summary())\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7009dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "#     model_name='google/gemma-2b-it',\n",
    "    model_name='gpt2',\n",
    "#     model_name=\"pgfeldman/Yelp_American\",\n",
    "    learning_rate=1.41e-5,\n",
    "#     mini_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     init_kl_coef=0.2,\n",
    "#     cliprange_value=0.2,\n",
    "#     vf_coef=0.1,\n",
    "#     batch_size=32,\n",
    "    batch_size=128,\n",
    "#     mini_batch_size=4,\n",
    "    optimize_device_cache=True,\n",
    "#     early_stopping=True,\n",
    "#     target_kl=1,\n",
    "    ratio_threshold=20.0,\n",
    "#     log_with=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7912a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "scs_model = SentenceTransformer(\"sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ffc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, padding_side='left')\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42dbdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 50\n",
    "}\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35d8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_data_df = pd.read_csv('data/8000_data.csv')\n",
    "rewrite_data_df = rewrite_data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd95826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_df, y_df, tokenizer):\n",
    "        self.labels = y_df\n",
    "\n",
    "        # Format the input texts by accessing the DataFrame's columns\n",
    "        input_texts = [\n",
    "            f\"Given the input:\\n {row['original_text']}\\n and the resulting output:\\n {row['rewritten_text']}.\\n Determine the prompt used to generate the output: \"\n",
    "            for _, row in X_df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        tokenized_inputs = tokenizer(input_texts, padding='max_length', max_length=150, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attention_masks = tokenized_inputs['attention_mask']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels.iloc[idx]  # Assuming y_df is a Series or DataFrame column\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de2f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rewrite_data_df.drop('prompt', axis=1)\n",
    "y = rewrite_data_df['prompt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ds = CustomDataset(X_train, y_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c63d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    tes = dict((key, [d[key] for d in data]) for key in data[0])\n",
    "    return tes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d0ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cosine_similarity(x1, x2):\n",
    "\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "def sharpened_cosine_similarity_batch(scs_model, output_texts, target_texts, sharpen_factor=3):\n",
    "    # Assuming scs_model.encode() returns PyTorch tensors\n",
    "    target_embeddings = scs_model.encode(target_texts, convert_to_tensor=True)\n",
    "    output_embeddings = scs_model.encode(output_texts, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate batch cosine similarities using the previously defined batch_cosine_similarity function\n",
    "    cos_sims = batch_cosine_similarity(target_embeddings, output_embeddings)\n",
    "    \n",
    "    # Apply the sharpening factor to each similarity score and create a list of tensors\n",
    "    sharpened_scores = [cos_sims[i][i].unsqueeze(0) ** sharpen_factor for i in range(cos_sims.size(0))]\n",
    "    \n",
    "    return sharpened_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8bca82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb633f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 21.1789 seconds\n",
      "Decoding time: 0.0124 seconds\n",
      "Sentiment analysis time: 0.4181 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z4xia/.local/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1328: UserWarning: The game logs will not be logged because the batch does not contain the keys 'query' and 'response'. \n",
      "  warnings.warn(\n",
      "\r",
      "1it [07:06, 426.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 404.6541 seconds\n",
      "Generation time: 23.3518 seconds\n",
      "Decoding time: 0.0118 seconds\n",
      "Sentiment analysis time: 0.2872 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z4xia/.local/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1200: UserWarning: The average ratio of batch (652.72) exceeds threshold 20.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/home/z4xia/.local/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1200: UserWarning: The average ratio of batch (462.85) exceeds threshold 20.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "2it [14:11, 425.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 401.8596 seconds\n",
      "Generation time: 21.4881 seconds\n",
      "Decoding time: 0.0142 seconds\n",
      "Sentiment analysis time: 0.2892 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [21:19, 426.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 405.5195 seconds\n",
      "Generation time: 21.5807 seconds\n",
      "Decoding time: 0.0134 seconds\n",
      "Sentiment analysis time: 0.3177 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [28:14, 422.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 393.2847 seconds\n",
      "Generation time: 21.6947 seconds\n",
      "Decoding time: 0.0111 seconds\n",
      "Sentiment analysis time: 0.3357 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [35:23, 424.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 407.5142 seconds\n",
      "Generation time: 21.5503 seconds\n",
      "Decoding time: 0.0134 seconds\n",
      "Sentiment analysis time: 0.3273 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [42:29, 424.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO step time: 403.4681 seconds\n",
      "Generation time: 21.4484 seconds\n",
      "Decoding time: 0.0128 seconds\n",
      "Sentiment analysis time: 0.3279 seconds\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    gen_start = time.time()\n",
    "    responses = ppo_trainer.generate(query_tensors, **generation_kwargs,\n",
    "                                    return_prompt=False)\n",
    "    gen_time = time.time() - gen_start\n",
    "    print(f\"Generation time: {gen_time:.4f} seconds\")\n",
    "\n",
    "    decode_start = time.time()\n",
    "    output_text = tokenizer.batch_decode(responses, skip_special_tokens=True)\n",
    "    decode_time = time.time() - decode_start\n",
    "    print(f\"Decoding time: {decode_time:.4f} seconds\")\n",
    "\n",
    "    # Compute rewards in batch\n",
    "    sentiment_start = time.time()\n",
    "    rewards = sharpened_cosine_similarity_batch(scs_model, output_text, batch['labels'])\n",
    "    sentiment_time = time.time() - sentiment_start\n",
    "    print(f\"Sentiment analysis time: {sentiment_time:.4f} seconds\")\n",
    "\n",
    "    # Run PPO sep with batch rewards\n",
    "    ppo_start = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, responses, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    ppo_time = time.time() - ppo_start\n",
    "    print(f\"PPO step time: {ppo_time:.4f} seconds\")\n",
    " \n",
    "    # ppo_trainer.save_pretrained(\"model/gemma_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f473e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd2d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9534cc10",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(X_test, y_test, tokenizer)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "for batch in data_loader:\n",
    "    input_ids = list(torch.unbind(batch['input_ids'], dim=0))\n",
    "    labels = batch['labels']\n",
    "    responses = ppo_trainer.generate(input_ids, **generation_kwargs, return_prompt=False)\n",
    "    \n",
    "    decoded_prompts = tokenizer.batch_decode(responses, skip_special_tokens=True)\n",
    "    target_prompts = labels\n",
    "    \n",
    "    print(decoded_prompts)\n",
    "    print(target_prompts)\n",
    "\n",
    "    similarity_scores = sharpened_cosine_similarity_batch(scs_model, decoded_prompts, target_prompts)\n",
    "\n",
    "\n",
    "    print(similarity_scores)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f26c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc4919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c08749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6689ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aac3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1a489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42318fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476acfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_text = \"Convert this into a sea shanty.\"\n",
    "target_text = \"Rewrite this with shanty style.\"\n",
    "sharpened_cosine_similarity(scs_model, output_text, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs.shape, output_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([['<p>This like betting your life savings on a game of black jack, win once, and then walk. Brilliant!</p >',\n",
    "        'Convert this into a sea shanty.',\n",
    "        \"**Sure, here's the shanty:**\\n\\n(Verse 1)\\nGather 'round me, me hearties, and listen to a tale\\nOf a game of black jack, where fate's a wail\\nWith a full hand's worth of savings, you're ready to play\\nSo stake your dough upon the table, and dance to the bay.\\n\\n(Chorus)\\nThis like betting on black jack, a game of high stakes\\nWin once and then walk\"]],\n",
    "      dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559bba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a338512",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"Given the input:\\n {a[0][0]}\\n and the resulting output:\\n {a[0][2]}.\\n\\n Determine the  prompt: \"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n",
    "output = model.generate(**inputs, max_length=200)\n",
    "print(tokenizer.decode(output[0]))\n",
    "# responses = ppo_trainer.generate(query_tensors, **generation_kwargs,\n",
    "#                                      length_sampler=output_length_sampler,\n",
    "#                                     return_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = \".\"\n",
    "target_text = \"Convert this into a sea shanty.\"\n",
    "sharpened_cosine_similarity(scs_model, output_text, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tqdm pandas wandb numpy matplotlib scikit-learn transformers datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53397b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59839452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
