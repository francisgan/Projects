{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GLw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>combined_text_inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I get it now. You have to give to get. Where's...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>\"The cosmic tapestry of existence unraveled be...</td>\n",
       "      <td>Original text: I get it now. You have to give ...</td>\n",
       "      <td>Original text: I get it now. You have to give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlas was not seen to be special by any means....</td>\n",
       "      <td>Restyle this text as if it were written by a A...</td>\n",
       "      <td>Restyle this text as if it were written by a A...</td>\n",
       "      <td>In the radiant tapestry of the summertime sky,...</td>\n",
       "      <td>Original text: Atlas was not seen to be specia...</td>\n",
       "      <td>Original text: Atlas was not seen to be specia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gleaming eyes shining in the dark.  Visions of...</td>\n",
       "      <td>Adapt this text as a script for a wizard in a ...</td>\n",
       "      <td>Adapt this text as a script for a wizard in a ...</td>\n",
       "      <td>(A smoky bar in the heart of a roaring twentie...</td>\n",
       "      <td>Original text: Gleaming eyes shining in the da...</td>\n",
       "      <td>Original text: Gleaming eyes shining in the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remember, there's security cameras at every co...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>The Time Traveler's Tale:\"My dear reader, I ha...</td>\n",
       "      <td>Original text: Remember, there's security came...</td>\n",
       "      <td>Original text: Remember, there's security came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waves  17 Septillion ships per cloud  Like loc...</td>\n",
       "      <td>Imagine this text was a villain in the world o...</td>\n",
       "      <td>Imagine this text was a villain in the world o...</td>\n",
       "      <td>Sure, here is the text rewritten as a villain ...</td>\n",
       "      <td>Original text: Waves  17 Septillion ships per ...</td>\n",
       "      <td>Original text: Waves  17 Septillion ships per ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  I get it now. You have to give to get. Where's...   \n",
       "1  Atlas was not seen to be special by any means....   \n",
       "2  Gleaming eyes shining in the dark.  Visions of...   \n",
       "3  Remember, there's security cameras at every co...   \n",
       "4  Waves  17 Septillion ships per cloud  Like loc...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Convey the same message as this text but throu...   \n",
       "1  Restyle this text as if it were written by a A...   \n",
       "2  Adapt this text as a script for a wizard in a ...   \n",
       "3  Convey the same message as this text but throu...   \n",
       "4  Imagine this text was a villain in the world o...   \n",
       "\n",
       "                                      rewrite_prompt  \\\n",
       "0  Convey the same message as this text but throu...   \n",
       "1  Restyle this text as if it were written by a A...   \n",
       "2  Adapt this text as a script for a wizard in a ...   \n",
       "3  Convey the same message as this text but throu...   \n",
       "4  Imagine this text was a villain in the world o...   \n",
       "\n",
       "                                      rewritten_text  \\\n",
       "0  \"The cosmic tapestry of existence unraveled be...   \n",
       "1  In the radiant tapestry of the summertime sky,...   \n",
       "2  (A smoky bar in the heart of a roaring twentie...   \n",
       "3  The Time Traveler's Tale:\"My dear reader, I ha...   \n",
       "4  Sure, here is the text rewritten as a villain ...   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Original text: I get it now. You have to give ...   \n",
       "1  Original text: Atlas was not seen to be specia...   \n",
       "2  Original text: Gleaming eyes shining in the da...   \n",
       "3  Original text: Remember, there's security came...   \n",
       "4  Original text: Waves  17 Septillion ships per ...   \n",
       "\n",
       "                                   combined_text_inf  \n",
       "0  Original text: I get it now. You have to give ...  \n",
       "1  Original text: Atlas was not seen to be specia...  \n",
       "2  Original text: Gleaming eyes shining in the da...  \n",
       "3  Original text: Remember, there's security came...  \n",
       "4  Original text: Waves  17 Septillion ships per ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "    df['combined_text'] = \"<|original|> Original text: \"+ df['original_text'] + \" <|rewritten|> Rewritten text:\" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \" + df['prompt']\n",
    "    df['combined_text_inf'] = \"<|original|> Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \"\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to each row in the 'rewritten_text' column\n",
    "df = add_special_tokens(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'fine_tune_data.txt'\n",
    "df['combined_text'][:980].to_csv(data_file, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7600c3c6ec746dc8c61432945fd73f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "data_file = 'fine_tune_data.txt'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "# Ensure the tokenizer will not split <|endoftext|> into multiple tokens\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<|endoftext|>']})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 3. Create the dataset and data collator\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=data_file,\n",
    "    block_size=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlgan\u001b[0m (\u001b[33mcse151b_llm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/gl_tets/wandb/run-20240317_004354-wpv6oinv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse151b_llm/huggingface/runs/wpv6oinv' target=\"_blank\">vocal-paper-24</a></strong> to <a href='https://wandb.ai/cse151b_llm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse151b_llm/huggingface' target=\"_blank\">https://wandb.ai/cse151b_llm/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse151b_llm/huggingface/runs/wpv6oinv' target=\"_blank\">https://wandb.ai/cse151b_llm/huggingface/runs/wpv6oinv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1840' max='1840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1840/1840 20:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1840, training_loss=0.9921675656152809, metrics={'train_runtime': 1237.2779, 'train_samples_per_second': 11.893, 'train_steps_per_second': 1.487, 'total_flos': 2.239751361134592e+16, 'train_loss': 0.9921675656152809, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    overwrite_output_dir=True,       # overwrite the content of the output directory\n",
    "    num_train_epochs=5,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    logging_steps=100,\n",
    "    save_steps=10_000,               # after # steps model is saved\n",
    "    save_total_limit=2,              # only last 2 models are saved\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# 5. Start the fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/gemma/1000_5e/tokenizer_config.json',\n",
       " 'model/gemma/1000_5e/special_tokens_map.json',\n",
       " 'model/gemma/1000_5e/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.save_pretrained('model/gemma/1000_5e/')\n",
    "tokenizer.save_pretrained('model/gemma/1000_5e/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('model/gemma/1000_5e/')\n",
    "model = AutoModelForCausalLM.from_pretrained('model/gemma/1000_5e/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256001, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original text: A 100 year war between me and my government.   Ends today.   With my victory.   They say powers are a curse, a virus infecting every human on earth. Later the boom waits the more deadly it is.   I am 100 years old to this day and im tired of being on the run, it\\'s time for me to go up in their front base and activate my power right then and there. I will be so strong that they ca n\\'t even imprison me.   24 hours later, the man that had written this letter was indeed telling a half truth. He lost that day, he did what he promised and went to their base and activated the power of....immortality.   They say you can hear his ancient cries from a mile away. <|endoftext|> Rewritten text: MOONLIT GRAVEYARD - NIGHTA lone grave stands in the center of a moonlit graveyard. The wind whips around the tomb, carrying with it the scent of decay and evil.VOICE (O.S.): (singsong voice) \"A 100-year war... ends today... with my victory...\"The voice echoes through the graveyard. It is the voice of a villain named GHOST, his tone dripping with hate and desperation.GHOST emerges from the tomb, his skeletal hand reaching out to the sky. He is shrouded in an eerie mist, and his eyes burn with an unnatural glow.GHOST (cont.) \"They say powers are a curse... a virus infecting every human on earth. Later the boom... waits... the more deadly it is.\"GHOST raises his hand and cracks the earth beneath him. The<|endoftext|> The prompt used to generate rewritten text given orignal text is: '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nindex = 983\n",
    "input_text = df[\"combined_text_inf\"][nindex]\n",
    "prompt = df[\"prompt\"][nindex]\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = 'Original text: If you\\'re happy and you know it clap your hands.    `` Dina and Friends\\'\\' was filmed in a real farm, because the directors wanted to give it a more realistic feeling.    If you\\'re happy and you know it clap your hands.    The goats and pigs moved expectantly towards Dina, bleating and  oinking for food.    If you\\'re happy and you know it...    The bright sun in the blue, cloudless sky was enough to make anyone sweat, especially if they were wearing a giant dinosaur costume.   ... then your face will surely show it.    `` Bang!\\'\\'    The original writer of `` Dina and Friends\\'\\' look down at the man in the dinosaur costume. He may not have his home, wife, or kids anymore, but his former co-worker was the one who had lost.    If you\\'re happy and you now it clap your hands.     -002  <|endoftext|> Rewritten text:  A farm in the sun. The sky is blue and cloudless.Narrator: (Singing) If you\\'re happy and you know it, clap your hands.Director: (To camera) \"Dina and Friends\" was filmed on a real farm to give it a more realistic feeling.Narrator: (Singing) If you\\'re happy and you know it, clap your hands.Goats and pigs: Move expectantly towards Dina, bleating and oinking for food.Narrator: (Singing) If you\\'re happy and you know it...Sun: Is shining brightly in the sky.Giant dinosaur costume: Makes anyone sweat, especially if they are wearing one.Giant dinosaur costume: Makes a loud noise and a lot of dust.Original writer: Looks down at the man in the dinosaur costume.<|endoftext|> The prompt used to generate rewritten text given orignal text is: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#input_text = \"I get it now. You have to give to get. Where's did I put the cereal? You ca n't have life without death. Or love without hate. I think this milk is sour. Maybe it's fine. Maybe I'm fine, just the way I am.   I have to shower before I go. Maybe the only reason everything is so shit is because I expect it to be. A positive attitude might be the only thing I need. Shit, I'm going to be late.   Learn to drive, asshole! I need to relax, like in that dream, some real Zen stuff. Love and equality and all that. Fuck, I spilled my coffee. Today is going to be hell.<|endoftext|> The cosmic tapestry of existence unraveled before me, illuminating the interconnectedness of all phenomena. I perceive the enigmatic axiom woven through the threads of life and death, love and hate. It is an axiom inscribed in the cosmic consciousness, a testament to the duality of existence.Through the prism of perception, I discern the paradoxical dance of giving and receiving. To acquire the sustenance necessary for survival, one must offer a reciprocal gift, a harmonious exchange of energy. The cosmic rhythm dictates that to emerge unscathed, one must embrace the yin and yang of life, death, and all that encompasses them.The sourness of the milk serves as a reminder of the impermanence of all temporal endeavors. Yet, within the abyss of doubt, I find solace in the recognition of my own intrinsic wholeness. Perhaps, the paradox of existence is reconciled through a profound sense of self-<|endoftext|>\"  # This is your input text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Adjust the max_length parameter as needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m output_sequences \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      7\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m      8\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Adjust the max length of the output text if needed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Number of output sequences to generate\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#input_text = \"I get it now. You have to give to get. Where's did I put the cereal? You ca n't have life without death. Or love without hate. I think this milk is sour. Maybe it's fine. Maybe I'm fine, just the way I am.   I have to shower before I go. Maybe the only reason everything is so shit is because I expect it to be. A positive attitude might be the only thing I need. Shit, I'm going to be late.   Learn to drive, asshole! I need to relax, like in that dream, some real Zen stuff. Love and equality and all that. Fuck, I spilled my coffee. Today is going to be hell.<|endoftext|> The cosmic tapestry of existence unraveled before me, illuminating the interconnectedness of all phenomena. I perceive the enigmatic axiom woven through the threads of life and death, love and hate. It is an axiom inscribed in the cosmic consciousness, a testament to the duality of existence.Through the prism of perception, I discern the paradoxical dance of giving and receiving. To acquire the sustenance necessary for survival, one must offer a reciprocal gift, a harmonious exchange of energy. The cosmic rhythm dictates that to emerge unscathed, one must embrace the yin and yang of life, death, and all that encompasses them.The sourness of the milk serves as a reminder of the impermanence of all temporal endeavors. Yet, within the abyss of doubt, I find solace in the recognition of my own intrinsic wholeness. Perhaps, the paradox of existence is reconciled through a profound sense of self-<|endoftext|>\"  # This is your input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text using the model\n",
    "# Adjust the max_length parameter as needed\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=len(input_ids[0])+30,  # Adjust the max length of the output text if needed\n",
    "    temperature=1.0,  # Temperature controls the randomness of the output\n",
    "    top_k=50,  # Top-k sampling\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1  # Number of output sequences to generate\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original text: If you\\'re happy and you know it clap your hands.    `` Dina and Friends\\'\\' was filmed in a real farm, because the directors wanted to give it a more realistic feeling.    If you\\'re happy and you know it clap your hands.    The goats and pigs moved expectantly towards Dina, bleating and  oinking for food.    If you\\'re happy and you know it...    The bright sun in the blue, cloudless sky was enough to make anyone sweat, especially if they were wearing a giant dinosaur costume.   ... then your face will surely show it.    `` Bang!\\'\\'    The original writer of `` Dina and Friends\\'\\' look down at the man in the dinosaur costume. He may not have his home, wife, or kids anymore, but his former co-worker was the one who had lost.    If you\\'re happy and you now it clap your hands.     -002  <|endoftext|> Rewritten text: Scene: A farm in the sun. The sky is blue and cloudless.Narrator: (Singing) If you\\'re happy and you know it, clap your hands.Director: (To camera) \"Dina and Friends\" was filmed on a real farm to give it a more realistic feeling.Narrator: (Singing) If you\\'re happy and you know it, clap your hands.Goats and pigs: Move expectantly towards Dina, bleating and oinking for food.Narrator: (Singing) If you\\'re happy and you know it...Sun: Is shining brightly in the sky.Giant dinosaur costume: Makes anyone sweat, especially if they are wearing one.Giant dinosaur costume: Makes a loud noise and a lot of dust.Original writer: Looks down at the man in the dinosaur costume.<|endoftext|> The prompt used to generate rewritten text given orignal text is: '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adapt this text as a script for a knight in a scientist setting.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8089], device='cuda:0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\"Adapt this text as a script for a romance novel in a knight setting.\"]\n",
    "b=[prompt]\n",
    "\n",
    "sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "scs_model = SentenceTransformer(\"sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  batch_cosine_similarity(x1, x2):\n",
    "\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "def sharpened_cosine_similarity_batch(scs_model, output_texts, target_texts, sharpen_factor=3):\n",
    "    # Assuming scs_model.encode() returns PyTorch tensors\n",
    "    target_embeddings = scs_model.encode(target_texts, convert_to_tensor=True)\n",
    "    output_embeddings = scs_model.encode(output_texts, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate batch cosine similarities using the previously defined batch_cosine_similarity function\n",
    "    cos_sims = batch_cosine_similarity(target_embeddings, output_embeddings)\n",
    "    \n",
    "    # Apply the sharpening factor to each similarity score and create a list of tensors\n",
    "    sharpened_scores = [cos_sims[i][i].unsqueeze(0) ** sharpen_factor for i in range(cos_sims.size(0))]\n",
    "    \n",
    "    return sharpened_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.6031], device='cuda:0')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
