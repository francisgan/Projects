{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38533b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 14:09:27.030654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer,GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f982ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Releases all unused cached memory from PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80cc75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bcf73e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ForumTopicId</th>\n",
       "      <th>PostUserId</th>\n",
       "      <th>PostDate</th>\n",
       "      <th>ReplyToForumMessageId</th>\n",
       "      <th>Message</th>\n",
       "      <th>Medal</th>\n",
       "      <th>MedalAwardDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>667073</td>\n",
       "      <td>116025</td>\n",
       "      <td>1666986</td>\n",
       "      <td>11/06/2019 19:33:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;This like betting your life savings on a ga...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>667072</td>\n",
       "      <td>111906</td>\n",
       "      <td>2982285</td>\n",
       "      <td>11/06/2019 19:31:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Hi everyone... I'm looking for a team. I'm ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>667071</td>\n",
       "      <td>114724</td>\n",
       "      <td>2468954</td>\n",
       "      <td>11/06/2019 19:29:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Thanks for sharing!&lt;/p&gt;</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>667070</td>\n",
       "      <td>116049</td>\n",
       "      <td>603584</td>\n",
       "      <td>11/06/2019 19:27:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;This competition's final submission deadlin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>667069</td>\n",
       "      <td>116025</td>\n",
       "      <td>2490236</td>\n",
       "      <td>11/06/2019 19:26:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;funny, it didn't work for me - got 13.837.\\...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11/06/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  ForumTopicId  PostUserId             PostDate  \\\n",
       "0  667073        116025     1666986  11/06/2019 19:33:54   \n",
       "1  667072        111906     2982285  11/06/2019 19:31:42   \n",
       "2  667071        114724     2468954  11/06/2019 19:29:30   \n",
       "3  667070        116049      603584  11/06/2019 19:27:39   \n",
       "4  667069        116025     2490236  11/06/2019 19:26:48   \n",
       "\n",
       "   ReplyToForumMessageId                                            Message  \\\n",
       "0                    NaN  <p>This like betting your life savings on a ga...   \n",
       "1                    NaN  <p>Hi everyone... I'm looking for a team. I'm ...   \n",
       "2                    NaN                         <p>Thanks for sharing!</p>   \n",
       "3                    NaN  <p>This competition's final submission deadlin...   \n",
       "4                    NaN  <p>funny, it didn't work for me - got 13.837.\\...   \n",
       "\n",
       "   Medal MedalAwardDate  \n",
       "0    3.0     11/06/2019  \n",
       "1    NaN            NaN  \n",
       "2    3.0     11/07/2019  \n",
       "3    1.0     11/07/2019  \n",
       "4    3.0     11/06/2019  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "forum_messsages_df = pd.read_csv('ForumMessages.csv')\n",
    "forum_messsages_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23c76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ae98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2429a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d4d43d056e43688a7ae3da78f90858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e35bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = forum_messsages_df['Message'][:10]\n",
    "rewrite_prompts = [\n",
    "    'Explain this to me like I\\'m five.',\n",
    "    'Convert this into a sea shanty.',\n",
    "    'Make this rhyme.',\n",
    "    \"Explain this to me as if it were a fairy tale.\",\n",
    "    \"Summarize this like it's a blockbuster movie trailer.\",\n",
    "    \"Tell me this story as if it were a detective novel.\",\n",
    "    \"Describe this as if it were a recipe in a cookbook.\",\n",
    "    \"Reimagine this as a letter from a soldier in a war.\",\n",
    "    \"Frame this as a dialogue between two historical figures.\",\n",
    "    \"Present this as if it were a debate between two philosophers.\",\n",
    "    \"Convey this as if it were a bedtime story for a child.\",\n",
    "    \"Illustrate this as if it were a scene from a silent film.\",\n",
    "    \"Narrate this as if it were a news report from the future.\",\n",
    "]\n",
    "len(rewrite_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f34a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:24<00:00,  8.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "# This is the prompt format the model expects\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "rewrite_data = []\n",
    "for original_text in tqdm(original_texts):\n",
    "    sampled_prompts = random.sample(rewrite_prompts,3)\n",
    "    for rewrite_prompt in sampled_prompts:\n",
    "        prompt = USER_CHAT_TEMPLATE.format(prompt=f'{rewrite_prompt}\\n{original_text}')\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate text\n",
    "        output_ids = model.generate(input_ids, max_length=100 + len(input_ids[0]))[0]\n",
    "\n",
    "        # Decode the generated ids to text\n",
    "        rewritten_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        rewrite_data.append({\n",
    "            'original_text': original_text,\n",
    "            'rewrite_prompt': rewrite_prompt,\n",
    "            'rewritten_text': rewritten_text.split('model\\n')[-1],\n",
    "        })\n",
    "\n",
    "rewrite_data_df = pd.DataFrame(rewrite_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Process in batch, requires more cuda space\n",
    "\n",
    "\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# random.seed(0)\n",
    "\n",
    "# # Assuming `original_texts` is a list of texts you want to process\n",
    "# # Assuming `tokenizer` and `model` have been properly initialized and moved to the GPU\n",
    "\n",
    "# USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "# batch_size = 4  # Adjust based on your GPU memory\n",
    "\n",
    "# rewrite_data = []\n",
    "\n",
    "# # Function to process texts in batches\n",
    "# def process_batch(batch):\n",
    "#     batch_outputs = []\n",
    "#     rewrite_prompts_batch = [random.choices(rewrite_prompts, k=3) for _ in range(len(batch))]\n",
    "#     for original_text, rewrite_prompts_for_text in zip(batch, rewrite_prompts_batch):\n",
    "#         for rewrite_prompt in rewrite_prompts_for_text:\n",
    "#             prompt = USER_CHAT_TEMPLATE.format(prompt=f'{rewrite_prompt}\\n{original_text}')\n",
    "#             batch_outputs.append((original_text, rewrite_prompt, prompt))\n",
    "#     return batch_outputs\n",
    "\n",
    "# # Generate batches of texts\n",
    "# batches = [original_texts[i:i + batch_size] for i in range(0, len(original_texts), batch_size)]\n",
    "\n",
    "# for batch in tqdm(batches):\n",
    "#     batch_outputs = process_batch(batch)\n",
    "#     prompts = [output[2] for output in batch_outputs]\n",
    "    \n",
    "#     # Tokenize all prompts in the batch\n",
    "#     input_ids = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "#     # Generate text for the entire batch\n",
    "#     output_ids = model.generate(input_ids[\"input_ids\"], max_length=100 + input_ids[\"input_ids\"].shape[1])\n",
    "\n",
    "#     # Decode the generated ids to text and append to rewrite_data\n",
    "#     for i, output_id in enumerate(output_ids):\n",
    "#         original_text, rewrite_prompt, _ = batch_outputs[i]\n",
    "#         rewritten_text = tokenizer.decode(output_id, skip_special_tokens=True).split('model\\n')[1]\n",
    "#         rewrite_data.append({\n",
    "#             'original_text': original_text,\n",
    "#             'rewrite_prompt': rewrite_prompt,\n",
    "#             'rewritten_text': rewritten_text,\n",
    "#         })\n",
    "\n",
    "# rewrite_data_df = pd.DataFrame(rewrite_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef4600b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_data_df['rewrite_prompt'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_data_df.to_csv('data/rewrite_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite_data_df = pd.read_csv('data/rewrite_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ce915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
