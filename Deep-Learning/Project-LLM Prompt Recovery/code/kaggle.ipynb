{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-15T07:39:31.896592Z",
     "iopub.status.busy": "2024-03-15T07:39:31.896205Z",
     "iopub.status.idle": "2024-03-15T07:39:50.469326Z",
     "shell.execute_reply": "2024-03-15T07:39:50.468384Z",
     "shell.execute_reply.started": "2024-03-15T07:39:31.896561Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer,GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:54:01.180298Z",
     "iopub.status.busy": "2024-03-15T07:54:01.179446Z",
     "iopub.status.idle": "2024-03-15T07:54:48.582388Z",
     "shell.execute_reply": "2024-03-15T07:54:48.581300Z",
     "shell.execute_reply.started": "2024-03-15T07:54:01.180270Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:15.501259Z",
     "iopub.status.busy": "2024-03-15T08:06:15.500308Z",
     "iopub.status.idle": "2024-03-15T08:06:15.678787Z",
     "shell.execute_reply": "2024-03-15T08:06:15.677952Z",
     "shell.execute_reply.started": "2024-03-15T08:06:15.501225Z"
    }
   },
   "outputs": [],
   "source": [
    "### Comment it out in Kaggle\n",
    "\n",
    "# train_df = pd.read_csv('/kaggle/input/8000-data/8000_data (1).csv')\n",
    "train_df = pd.read_csv('data/8000_data.csv')\n",
    "# train_df = pd.read_csv('data/generate_data.csv')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly shuffling the dataframe\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df.rename(columns={'prompt':'rewrite_prompt'})\n",
    "train_df['id'] = range(len(train_df))\n",
    "test_df = train_df[1000:2000]\n",
    "train_df = train_df[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:16.030244Z",
     "iopub.status.busy": "2024-03-15T08:06:16.029900Z",
     "iopub.status.idle": "2024-03-15T08:06:16.037826Z",
     "shell.execute_reply": "2024-03-15T08:06:16.036850Z",
     "shell.execute_reply.started": "2024-03-15T08:06:16.030219Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "# test_df = test_df.dropna()\n",
    "test_df['original_text'] = test_df['original_text'].fillna('')\n",
    "test_df['rewritten_text'] = test_df['rewritten_text'].fillna('')\n",
    "\n",
    "train_df['original_text'] = train_df['original_text'].apply(lambda x: x[:200])\n",
    "train_df['rewritten_text'] = train_df['rewritten_text'].apply(lambda x: x[:200])\n",
    "\n",
    "test_df['original_text'] = test_df['original_text'].apply(lambda x: x[:200])\n",
    "test_df['rewritten_text'] = test_df['rewritten_text'].apply(lambda x: x[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:16.599802Z",
     "iopub.status.busy": "2024-03-15T08:06:16.599083Z",
     "iopub.status.idle": "2024-03-15T08:06:16.617653Z",
     "shell.execute_reply": "2024-03-15T08:06:16.616795Z",
     "shell.execute_reply.started": "2024-03-15T08:06:16.599770Z"
    }
   },
   "outputs": [],
   "source": [
    "### Uncomment it\n",
    "\n",
    "# train_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/train.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv')\n",
    "\n",
    "train_df['word_count'] = train_df['rewrite_prompt'].apply(lambda x: len(x.split()))\n",
    "word_count_distribution = train_df['word_count'].value_counts(normalize=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:18.737442Z",
     "iopub.status.busy": "2024-03-15T08:06:18.736735Z",
     "iopub.status.idle": "2024-03-15T08:06:18.818924Z",
     "shell.execute_reply": "2024-03-15T08:06:18.817979Z",
     "shell.execute_reply.started": "2024-03-15T08:06:18.737406Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:20.051218Z",
     "iopub.status.busy": "2024-03-15T08:06:20.050327Z",
     "iopub.status.idle": "2024-03-15T08:06:20.066438Z",
     "shell.execute_reply": "2024-03-15T08:06:20.065576Z",
     "shell.execute_reply.started": "2024-03-15T08:06:20.051184Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MaskedSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, mask_token='[MASK]', distribution=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "        self.dataframe['original_text'] = dataframe['original_text'].apply(lambda x: re.sub('<.*?>', '', x).strip())\n",
    "        self.dataframe['rewritten_text'] = dataframe['rewritten_text'].apply(lambda x: re.sub('<.*?>', '', x).strip())\n",
    "        if distribution is None:\n",
    "            self.dataframe['rewrite_prompt'] = dataframe['rewrite_prompt'].apply(lambda x: re.sub('<.*?>', '', x).strip())\n",
    "        self.dataframe['id'] = dataframe['id']\n",
    "        self.distribution = distribution\n",
    "        \n",
    "    def combine_and_mask(self, original, rewrite, prompt_length):\n",
    "        masks = \" \".join([self.mask_token for _ in range(prompt_length)])\n",
    "#         masked_sequence = f\"{original} The task is to rewrite this narrative with the given blanks: {masks} {rewrite}\"\n",
    "        masked_sequence = f\"[CLS] {original} [SEP] The task is to rewrite this narrative with the given blanks: {masks} {rewrite} [SEP]\"\n",
    "\n",
    "        return masked_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        if self.distribution is None:\n",
    "            prompt = row['rewrite_prompt']\n",
    "            target_subsequence_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            mask_length = len(target_subsequence_ids)\n",
    "        else:\n",
    "            mask_length = np.random.choice(self.distribution.index, p=self.distribution.values, size=1)[0]\n",
    "        masked_sequence = self.combine_and_mask(row['original_text'], row['rewritten_text'], mask_length)\n",
    "        \n",
    "        inputs = self.tokenizer(masked_sequence, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        \n",
    "        labels = torch.full_like(input_ids, fill_value=-100)\n",
    "        \n",
    "        mask_indices = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=False).squeeze()\n",
    "        \n",
    "        if self.distribution is None:\n",
    "            if len(mask_indices) >= mask_length:\n",
    "                labels[mask_indices[:mask_length]] = torch.tensor(target_subsequence_ids, dtype=torch.long)\n",
    "            else:\n",
    "                raise ValueError(\"Not enough mask tokens to fit the rewrite prompt\")\n",
    "            return row['id'], input_ids, labels\n",
    "        else:\n",
    "            if len(mask_indices) >= mask_length:\n",
    "                labels[mask_indices[:mask_length]] = torch.ones(mask_length, dtype=torch.long)\n",
    "            else:\n",
    "                raise ValueError(\"Not enough mask tokens to fit the rewrite prompt\")\n",
    "            return row['id'], input_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:21.786557Z",
     "iopub.status.busy": "2024-03-15T08:06:21.786167Z",
     "iopub.status.idle": "2024-03-15T08:06:23.189128Z",
     "shell.execute_reply": "2024-03-15T08:06:23.188101Z",
     "shell.execute_reply.started": "2024-03-15T08:06:21.786527Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Uncomment on Kaggle to load BERT\n",
    "# MODEL_DIR = \"/kaggle/input/huggingface-bert/\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(MODEL_DIR + \"bert-base-uncased\")\n",
    "# model = BertForMaskedLM.from_pretrained(MODEL_DIR + \"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:24.508068Z",
     "iopub.status.busy": "2024-03-15T08:06:24.507695Z",
     "iopub.status.idle": "2024-03-15T08:06:24.533892Z",
     "shell.execute_reply": "2024-03-15T08:06:24.532830Z",
     "shell.execute_reply.started": "2024-03-15T08:06:24.508036Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = MaskedSequenceDataset(train_df, tokenizer)\n",
    "test_dataset = MaskedSequenceDataset(test_df, tokenizer, distribution=word_count_distribution)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:06:29.288736Z",
     "iopub.status.busy": "2024-03-15T08:06:29.287885Z",
     "iopub.status.idle": "2024-03-15T08:10:33.968805Z",
     "shell.execute_reply": "2024-03-15T08:10:33.967348Z",
     "shell.execute_reply.started": "2024-03-15T08:06:29.288695Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device) \n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
    "        _, input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average loss at epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-15T08:10:33.969525Z",
     "iopub.status.idle": "2024-03-15T08:10:33.969927Z",
     "shell.execute_reply": "2024-03-15T08:10:33.969758Z",
     "shell.execute_reply.started": "2024-03-15T08:10:33.969743Z"
    }
   },
   "outputs": [],
   "source": [
    "### Comment the block out in Kaggle\n",
    "\n",
    "def batch_cosine_similarity(x1, x2):\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    \n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "scs_model = SentenceTransformer(\"sentence-t5-base\")\n",
    "\n",
    "def sharpened_cosine_similarity_batch(scs_model, output_texts, target_texts, sharpen_factor=3):\n",
    "    target_embeddings = scs_model.encode(target_texts, convert_to_tensor=True).to(device)\n",
    "    output_embeddings = scs_model.encode(output_texts, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    cos_sims = batch_cosine_similarity(target_embeddings, output_embeddings)\n",
    "    \n",
    "    sharpened_scores = [cos_sims[i][i].unsqueeze(0) ** sharpen_factor for i in range(cos_sims.size(0))]\n",
    "    \n",
    "    return sharpened_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_texts = [\"Transform this sentence into a more humorous and sarcastic version, using irony and wit.\"]\n",
    "target_texts = [\"Rewrite this text to infuse it with humor and a light-hearted tone, while still conveying the scientific facts and the relative safety due to the distant future impact date.\"]\n",
    "sharpened_cosine_similarity_batch(scs_model, output_texts,target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-15T08:10:33.971577Z",
     "iopub.status.idle": "2024-03-15T08:10:33.972034Z",
     "shell.execute_reply": "2024-03-15T08:10:33.971810Z",
     "shell.execute_reply.started": "2024-03-15T08:10:33.971790Z"
    }
   },
   "outputs": [],
   "source": [
    "### Comment it out in Kagle\n",
    "\n",
    "# predicted_sentences = []\n",
    "# target_sentences = []\n",
    "# id_list = []\n",
    "\n",
    "# for batch in tqdm(test_loader):\n",
    "#     ids, input_ids, labels = batch\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "#         for idx, (pred, label, id_value) in enumerate(zip(predictions, labels, ids)):\n",
    "#             valid_positions = label != -100\n",
    "#             valid_predictions = pred[valid_positions]\n",
    "#             valid_label = label[valid_positions]\n",
    "#             predicted_sentence = tokenizer.decode(valid_predictions, skip_special_tokens=True)\n",
    "\n",
    "#             target_sentence = tokenizer.decode(valid_label, skip_special_tokens=True)\n",
    "        \n",
    "#             predicted_sentences.append(predicted_sentence)\n",
    "#             id_list.append(id_value.item())  \n",
    "#             target_sentences.append(target_sentence)\n",
    "# score = sharpened_cosine_similarity_batch(scs_model, predicted_sentences, target_sentences, sharpen_factor=3)\n",
    "# print(torch.mean(torch.stack(score)))\n",
    "# score         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:19:59.508482Z",
     "iopub.status.busy": "2024-03-15T08:19:59.507818Z",
     "iopub.status.idle": "2024-03-15T08:20:26.892368Z",
     "shell.execute_reply": "2024-03-15T08:20:26.891452Z",
     "shell.execute_reply.started": "2024-03-15T08:19:59.508450Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_sentences = []\n",
    "id_list = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    ids, input_ids, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "    for idx, (pred, label, id_value) in enumerate(zip(predictions, labels, ids)):\n",
    "        valid_positions = label != -100\n",
    "        valid_predictions = pred[valid_positions]\n",
    "        \n",
    "        predicted_sentence = tokenizer.decode(valid_predictions, skip_special_tokens=True)\n",
    "        \n",
    "        predicted_sentences.append(predicted_sentence)\n",
    "        id_list.append(id_value.item())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor(0.5840, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment it out in Kaggle\n",
    "label_sentences = test_df['rewrite_prompt'].tolist()\n",
    "score = sharpened_cosine_similarity_batch(scs_model, predicted_sentences, label_sentences, sharpen_factor=3)\n",
    "print(torch.mean(torch.stack(score)))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T08:20:34.283415Z",
     "iopub.status.busy": "2024-03-15T08:20:34.283012Z",
     "iopub.status.idle": "2024-03-15T08:20:34.298440Z",
     "shell.execute_reply": "2024-03-15T08:20:34.297185Z",
     "shell.execute_reply.started": "2024-03-15T08:20:34.283383Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'id': id_list,'rewrite_prompt': predicted_sentences})\n",
    "# submission_df = pd.DataFrame({'id': id_list,'rewrite_prompt': predicts, 'target':targets})\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4602540,
     "sourceId": 7848860,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
