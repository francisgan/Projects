{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d2d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 22:16:07.633379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer,GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb9e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I get it now. You have to give to get. Where's...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>\"The cosmic tapestry of existence unraveled be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlas was not seen to be special by any means....</td>\n",
       "      <td>Restyle this text as if it were written by a A...</td>\n",
       "      <td>His emerald glow illuminated the celestial can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gleaming eyes shining in the dark.  Visions of...</td>\n",
       "      <td>Adapt this text as a script for a wizard in a ...</td>\n",
       "      <td>(A smoky bar in the heart of a roaring twentie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remember, there's security cameras at every co...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>\"My dear reader, I have come from the future t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waves  17 Septillion ships per cloud  Like loc...</td>\n",
       "      <td>Imagine this text was a villain in the world o...</td>\n",
       "      <td>If written as a villain in the world of villai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  I get it now. You have to give to get. Where's...   \n",
       "1  Atlas was not seen to be special by any means....   \n",
       "2  Gleaming eyes shining in the dark.  Visions of...   \n",
       "3  Remember, there's security cameras at every co...   \n",
       "4  Waves  17 Septillion ships per cloud  Like loc...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Convey the same message as this text but throu...   \n",
       "1  Restyle this text as if it were written by a A...   \n",
       "2  Adapt this text as a script for a wizard in a ...   \n",
       "3  Convey the same message as this text but throu...   \n",
       "4  Imagine this text was a villain in the world o...   \n",
       "\n",
       "                                      rewritten_text  \n",
       "0  \"The cosmic tapestry of existence unraveled be...  \n",
       "1  His emerald glow illuminated the celestial can...  \n",
       "2  (A smoky bar in the heart of a roaring twentie...  \n",
       "3  \"My dear reader, I have come from the future t...  \n",
       "4  If written as a villain in the world of villai...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_data_df = pd.read_csv('data/8000_data.csv')\n",
    "rewrite_data_df = rewrite_data_df.dropna()\n",
    "rewrite_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df3e7a",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a7fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, GPT2Model, GPT2Config, PreTrainedModel, BertTokenizer, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertGPT2PromptModel(PreTrainedModel):\n",
    "    def __init__(self, encoder_pretrained_model_name='bert-base-uncased', decoder_pretrained_model_name='gpt2'):\n",
    "        super(BertGPT2PromptModel, self).__init__(GPT2Config.from_pretrained(decoder_pretrained_model_name))\n",
    "        \n",
    "        self.encoder = BertModel.from_pretrained(encoder_pretrained_model_name)\n",
    "        self.decoder = GPT2Model.from_pretrained(decoder_pretrained_model_name)\n",
    "        \n",
    "        assert self.encoder.config.hidden_size == self.decoder.config.n_embd, \"Encoder and decoder hidden sizes do not match\"\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, use_teacher_forcing=False):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_last_hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Apply teacher forcing if enabled and decoder_input_ids are provided\n",
    "        if use_teacher_forcing and decoder_input_ids is not None:\n",
    "            # Here, we pass decoder_input_ids directly as input to the decoder\n",
    "            decoder_outputs = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask)\n",
    "        else:\n",
    "            # If not using teacher forcing, use the encoded states as inputs_embeds to the decoder\n",
    "            decoder_outputs = self.decoder(inputs_embeds=encoder_last_hidden_state, attention_mask=decoder_attention_mask)\n",
    "        \n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb47b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_prompt = lambda inp, out: f\"\"\"You are an intelligent prompt generator. I will provide you with an input and a llm-generated output. Your task is to identify the prompt that instructed an LLM to rewrite the input into the output. Please refer to the example below for the desired format.\n",
    "\n",
    "Input: \"This like betting your life savings on a game of black jack, win once, and then walk. Brilliant!\"\n",
    "Output: \"\\n\\n\\Good evening, and welcome to the future, folks. Tonight, we bring you a story that will have you on the edge of your seat. It\\'s a tale of a game that\\'s been captivating the world, and it\\'s one that you won\\'t believe is actually happening.\"\n",
    "Prompt: Narrate this as if it were a news report fromÂ future\n",
    "\n",
    "Input: \"The security sensors work on *sound*, though, not light. Regardless of what disguise you use to enter, they can catch you. \"\n",
    "Output: \"Imagine you're playing hide and seek, but instead of using your eyes to find someone, you use your ears to listen for sounds they make. The security sensors are like that; they listen for sounds to catch people, not look for them with eyes. So, even if you dress up differently, if you make a sound, they can still find you!\"\n",
    "Prompt: Explain this to me like I\\'m five\n",
    "        \n",
    "Input: \"{inp}\"\n",
    "Output: \"{out}\"\n",
    "Prompt:\"\"\".lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8dbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_name = 'bert-base-uncased'\n",
    "decoder_model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163438d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertGPT2PromptModel(encoder_pretrained_model_name=encoder_model_name, decoder_pretrained_model_name=decoder_model_name)\n",
    "tokenizer_encoder = BertTokenizer.from_pretrained(encoder_model_name)\n",
    "tokenizer_decoder = GPT2Tokenizer.from_pretrained(decoder_model_name)\n",
    "\n",
    "test_input = \"This like betting your life savings on a game of black jack, win once, and then walk. Brilliant!\"\n",
    "test_output = \"\\n\\n(Verse 1)\\nGather 'round me, me hearties, and listen to a tale\\nOf a game of black jack, where fate's a wail\\nWith a full hand's worth of savings, you're ready to play\\nSo stake your dough upon the table, and dance to the bay.\\n\\n(Chorus)\\nThis like betting on black jack, a game of high stakes\\nWin once and then walk\"\n",
    "\n",
    "# Create gpt_prompt\n",
    "test_prompt = gpt_prompt(test_input, test_output)\n",
    "\n",
    "# test_prompt = gpt_prompt(rewrite_data_df.iloc[0][0], rewrite_data_df.iloc[0][2])\n",
    "\n",
    "# Tokenize input \n",
    "input_ids = tokenizer_encoder(test_prompt, return_tensors='pt').input_ids\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "output_probs = outputs[0].argmax(-1)\n",
    "prompt_text = tokenizer_decoder.decode(output_probs.squeeze().tolist(), skip_special_tokens=True) #not predicting spaces\n",
    "prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b44bdd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageage'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probs = outputs[0].argmax(-1)\n",
    "prompt_text = tokenizer_decoder.decode(output_probs.squeeze().tolist(), skip_special_tokens=True) #not predicting spaces\n",
    "prompt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0ef16",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8287f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, bert_tokenizer, gpt2_tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if gpt2_tokenizer.pad_token is None:\n",
    "            gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "        self.bert_encoded = bert_tokenizer.batch_encode_plus(\n",
    "            dataframe['input'].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        self.gpt2_encoded = gpt2_tokenizer.batch_encode_plus(\n",
    "            dataframe['prompt'].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.bert_encoded['input_ids'][idx]\n",
    "        attention_mask = self.bert_encoded['attention_mask'][idx]\n",
    "\n",
    "        # For GPT-2, we create decoder_input_ids which is used for teacher forcing during training\n",
    "        decoder_input_ids = self.gpt2_encoded['input_ids'][idx][:-1]\n",
    "        labels = self.gpt2_encoded['input_ids'][idx][1:].clone().detach()\n",
    "\n",
    "        # Replace padding token id's of the labels by -100 so that they are not considered in the loss\n",
    "        labels[labels == gpt2_tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c67cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, use simple connecting. \n",
    "\n",
    "df = pd.read_csv(\"data/8000_data.csv\")\n",
    "df = df.dropna()\n",
    "df['input'] = df['original_text'] + ' ' + df['rewritten_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15ee7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['input']], df['prompt'], test_size=0.33, random_state=42)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "768d7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# bert_tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "# gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "train_dataset = TextDataset(train_data, bert_tokenizer, gpt2_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(test_data, bert_tokenizer, gpt2_tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a370c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "import torch\n",
    "\n",
    "class BertModelModified(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,  # Ensure that hidden states are returned\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Accessing the second-to-last hidden state, which are the inputs to the last layer\n",
    "        second_to_last_hidden_states = outputs.hidden_states[-2]\n",
    "\n",
    "        # You can modify this return statement based on what you need\n",
    "        # For instance, you might want to return just the last hidden state and the second-to-last hidden states\n",
    "        return outputs.last_hidden_state, second_to_last_hidden_states\n",
    "\n",
    "# Example usage\n",
    "config = BertConfig()\n",
    "bert_model_modified = BertModelModified(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d47d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_LABEL_LIMIT = 768\n",
    "REPLACE_TOKEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e610d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0844baf61bbb41f6b28fcd629b60532d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "# encoder = bert_model_modified #__main__.BertModelModified\n",
    "isinstance(encoder, transformers.models.bert.modeling_bert.BertModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5aa8e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, GPT2Model, GPT2Config, PreTrainedModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertGPT2PromptModel(PreTrainedModel):\n",
    "    def __init__(self, encoder, decoder_pretrained_model_name='gpt2'):\n",
    "        decoder_config = GPT2Config.from_pretrained(decoder_pretrained_model_name, add_cross_attention=True)\n",
    "        super(BertGPT2PromptModel, self).__init__(decoder_config)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = GPT2Model.from_pretrained(decoder_pretrained_model_name, config=decoder_config)\n",
    "        \n",
    "        assert self.encoder.config.hidden_size == self.decoder.config.n_embd, \"Encoder and decoder hidden sizes do not match\"\n",
    "        \n",
    "        # max pooling layer, trying to copy Bert's max pooling layer\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(self.decoder.config.n_embd) \n",
    "        \n",
    "        # fully connected layer to recover dimension\n",
    "        self.fc = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.n_embd)\n",
    "        self.start_token_id = self.decoder.config.eos_token_id\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, encoder_hidden_states=None, use_teacher_forcing=False):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if isinstance(self.encoder, BertModelModified):\n",
    "            _, second_to_last_hidden_states = encoder_outputs  \n",
    "            \n",
    "            # Apply global max pooling and the fully connected layer\n",
    "            #print(second_to_last_hidden_states.shape) # [2, 512, 768]\n",
    "            pooled_output = self.global_max_pool(second_to_last_hidden_states)#.squeeze(-1)  # Change dimensions for pooling\n",
    "            #print(pooled_output.shape) # [2, 512, 1]\n",
    "            \n",
    "            encoder_hidden_states = self.fc(pooled_output)\n",
    "            \n",
    "            \n",
    "        if use_teacher_forcing and decoder_input_ids is not None:\n",
    "            start_tokens = torch.ones((input_ids.size(0), 1), dtype=torch.long, device=input_ids.device) * self.start_token_id\n",
    "            decoder_input_ids = torch.cat((start_tokens, decoder_input_ids), dim=1)\n",
    "\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids, \n",
    "                attention_mask=decoder_attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states, # [2, 512, 1].view(-1, x.size(-1)) -> [1024, 1]\n",
    "                use_cache=False  # Turn off caching when using cross-attention\n",
    "            )\n",
    "        else:\n",
    "            # Initialize the list to store decoder outputs\n",
    "            decoder_output_sequences = []\n",
    "            max_length = 100\n",
    "\n",
    "            # Start token for the first input to the decoder\n",
    "            start_tokens = torch.ones((input_ids.size(0), 1), dtype=torch.long, device=input_ids.device) * self.start_token_id\n",
    "            decoder_input = start_tokens\n",
    "\n",
    "            # Loop to generate sequence\n",
    "            for _ in range(max_length):  # max_length is a predefined maximum length for generation\n",
    "                decoder_outputs = self.decoder(\n",
    "                    input_ids=decoder_input, \n",
    "                    attention_mask=decoder_attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    use_cache=False  # Turn off caching when using cross-attention\n",
    "                )\n",
    "\n",
    "                # Get the last token in the sequence\n",
    "                next_token_logits = decoder_outputs.last_hidden_state[:, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "                # Append generated token to the list\n",
    "                decoder_output_sequences.append(next_token)\n",
    "\n",
    "                # Prepare the next input token\n",
    "                decoder_input = torch.cat((decoder_input, next_token), dim=1)\n",
    "\n",
    "            # Concatenate the list of generated tokens to form the final sequence\n",
    "            decoder_outputs = torch.cat(decoder_output_sequences, dim=1)\n",
    "        \n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58b666f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1:  74%|ââââââââ  | 1909/2597 [03:29<01:15,  9.10batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_156/1027414352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#         for name, param in model.named_parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model = BertGPT2PromptModel(encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "\n",
    "total_loss = 0\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # this is where the problem is\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, use_teacher_forcing=True)\n",
    "\n",
    "        logits = outputs.last_hidden_state\n",
    "        logits = logits[:, :-1, :]\n",
    "\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        \n",
    "        labels[labels > BERT_LABEL_LIMIT] = REPLACE_TOKEN\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=-100) \n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "#         for name, param in model.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 if param.grad is None:\n",
    "#                     print(name)\n",
    "# #                 if torch.any(torch.isnan(param.grad)) or torch.any(torch.isinf(param.grad)):\n",
    "# #                     print(f\"NaN or Inf found in gradients of {name}\")\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77916d5",
   "metadata": {},
   "source": [
    "### Test one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbf8394e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageageraage Iageageage Iage I I I rageageageageageageageageraage I rageraraage I rraage I rrararararaage r I rage I rrararararararara'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BertGPT2PromptModel(encoder_pretrained_model_name=encoder_model_name, decoder_pretrained_model_name=decoder_model_name)\n",
    "tokenizer_encoder = BertTokenizer.from_pretrained(encoder_model_name)\n",
    "tokenizer_decoder = GPT2Tokenizer.from_pretrained(decoder_model_name)\n",
    "\n",
    "# test_input = \"This like betting your life savings on a game of black jack, win once, and then walk. Brilliant!\"\n",
    "# test_output = \"\\n\\n(Verse 1)\\nGather 'round me, me hearties, and listen to a tale\\nOf a game of black jack, where fate's a wail\\nWith a full hand's worth of savings, you're ready to play\\nSo stake your dough upon the table, and dance to the bay.\\n\\n(Chorus)\\nThis like betting on black jack, a game of high stakes\\nWin once and then walk\"\n",
    "\n",
    "# # Create gpt_prompt\n",
    "# test_prompt = gpt_prompt(test_input, test_output)\n",
    "\n",
    "# # test_prompt = gpt_prompt(rewrite_data_df.iloc[0][0], rewrite_data_df.iloc[0][2])\n",
    "\n",
    "# # Tokenize input \n",
    "# input_ids = tokenizer_encoder(test_prompt, return_tensors='pt').input_ids\n",
    "\n",
    "model.eval()\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, use_teacher_forcing=True)\n",
    "    \n",
    "output_probs = outputs[0]#.argmax(-1)\n",
    "prompt_text = tokenizer_decoder.decode(output_probs.squeeze().tolist(), skip_special_tokens=True) #not predicting spaces\n",
    "prompt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1946e",
   "metadata": {},
   "source": [
    "### Test for sharp cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d155ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cosine_similarity(x1, x2):\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea74b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:  # Assuming you have a DataLoader for your test data\n",
    "        \n",
    "        # Transfer the batch data to the same device as your model\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass: compute the model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, use_teacher_forcing=False)\n",
    "        \n",
    "        # Extract the logits\n",
    "        logits = outputs.last_hidden_state\n",
    "        logits = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        labels[labels > BERT_LABEL_LIMIT] = REPLACE_TOKEN  # Use the same token replacement as in training\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "        output_probs = outputs[0]#.argmax(-1)\n",
    "        prompt_text = tokenizer_decoder.decode(output_probs.squeeze().tolist(), skip_special_tokens=True) #not predicting spaces\n",
    "        \n",
    "        #TODO: calculate cosine similarity between prompt_text and actual prompt\n",
    "\n",
    "# Compute average test loss\n",
    "average_test_loss = total_test_loss / len(test_dataloader)\n",
    "print(f\"Average test loss: {average_test_loss}\")\n",
    "\n",
    "# Process your predictions as needed\n",
    "# For example, you might want to aggregate batch predictions, convert token IDs back to text, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6644d8",
   "metadata": {},
   "source": [
    "### experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [name for name, param in model.named_parameters()]\n",
    "torch.any(torch.isnan(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d31129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~0.9-2 percent of labels are larger than 768\n",
    "exceed = np.sum([int(ele) for ele in (labels.sort(descending=True).values > 768)])\n",
    "print(exceed)\n",
    "\n",
    "data = logits.unique().detach().numpy()\n",
    "\n",
    "plt.hist(data, bins=100, alpha=0.6, color='g')\n",
    "plt.title('Distribution of Tensor Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "sum(data == 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
