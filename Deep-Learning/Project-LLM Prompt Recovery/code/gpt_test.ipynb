{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GLw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>combined_text_inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hollis Price (born October 29, 1979) is an Ame...</td>\n",
       "      <td>Convert the text into a recipe, where each ins...</td>\n",
       "      <td>Hollis Price's Basketball DreamIngredients:- 1...</td>\n",
       "      <td>Original text: Hollis Price (born October 29, ...</td>\n",
       "      <td>Original text: Hollis Price (born October 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grangewoood Independent School is an independe...</td>\n",
       "      <td>Transform the text into a cryptic crossword pu...</td>\n",
       "      <td>Across1. A school situated in a London borough...</td>\n",
       "      <td>Original text: Grangewoood Independent School ...</td>\n",
       "      <td>Original text: Grangewoood Independent School ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tap... tap... tap. I could hear him hiding som...</td>\n",
       "      <td>Translate the essence of this text into a horr...</td>\n",
       "      <td>A tap, tap, tap echoed through the walls, send...</td>\n",
       "      <td>Original text: Tap... tap... tap. I could hear...</td>\n",
       "      <td>Original text: Tap... tap... tap. I could hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man smiled as he saw a package underneath hi...</td>\n",
       "      <td>Write like Oscar Wilde: Adopt Wilde's witty an...</td>\n",
       "      <td>With a groan, he hoisted the box, an embodimen...</td>\n",
       "      <td>Original text: A man smiled as he saw a packag...</td>\n",
       "      <td>Original text: A man smiled as he saw a packag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After weeks of making phone calls we finally f...</td>\n",
       "      <td>Rewrite this text in the style of a time trave...</td>\n",
       "      <td>Weeks of frantic phone calls yielded a celesti...</td>\n",
       "      <td>Original text: After weeks of making phone cal...</td>\n",
       "      <td>Original text: After weeks of making phone cal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Hollis Price (born October 29, 1979) is an Ame...   \n",
       "1  Grangewoood Independent School is an independe...   \n",
       "2  Tap... tap... tap. I could hear him hiding som...   \n",
       "3  A man smiled as he saw a package underneath hi...   \n",
       "4  After weeks of making phone calls we finally f...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Convert the text into a recipe, where each ins...   \n",
       "1  Transform the text into a cryptic crossword pu...   \n",
       "2  Translate the essence of this text into a horr...   \n",
       "3  Write like Oscar Wilde: Adopt Wilde's witty an...   \n",
       "4  Rewrite this text in the style of a time trave...   \n",
       "\n",
       "                                      rewritten_text  \\\n",
       "0  Hollis Price's Basketball DreamIngredients:- 1...   \n",
       "1  Across1. A school situated in a London borough...   \n",
       "2  A tap, tap, tap echoed through the walls, send...   \n",
       "3  With a groan, he hoisted the box, an embodimen...   \n",
       "4  Weeks of frantic phone calls yielded a celesti...   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Original text: Hollis Price (born October 29, ...   \n",
       "1  Original text: Grangewoood Independent School ...   \n",
       "2  Original text: Tap... tap... tap. I could hear...   \n",
       "3  Original text: A man smiled as he saw a packag...   \n",
       "4  Original text: After weeks of making phone cal...   \n",
       "\n",
       "                                   combined_text_inf  \n",
       "0  Original text: Hollis Price (born October 29, ...  \n",
       "1  Original text: Grangewoood Independent School ...  \n",
       "2  Original text: Tap... tap... tap. I could hear...  \n",
       "3  Original text: A man smiled as he saw a packag...  \n",
       "4  Original text: After weeks of making phone cal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "#    df['combined_text'] = \"<|original|> Original text: \"+ df['original_text'] + \" <|rewritten|> Rewritten text:\" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \" + df['prompt']\n",
    "#    df['combined_text_inf'] = \"<|original|> Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|prompt|>\" + \" The prompt that instructing language model to rewrite original text into rewritten text is: \"\n",
    "#    return df\n",
    "\n",
    "def add_special_tokens(df):\n",
    "    # Concatenate the prompts and texts with a special token in between\n",
    "    df['combined_text'] = \"Original text: \"+ df['original_text'] + \" <|endoftext|> Rewritten text:\" + df['rewritten_text'] + \"<|endoftext|> The prompt used to generate rewritten text given original text is: \" + df['prompt']\n",
    "    df['combined_text_inf'] = \"Original text: \" + df['original_text'] + \" <|endoftext|> Rewritten text: \" + df['rewritten_text'] + \"<|endoftext|> The prompt used to generate rewritten text given original text is: \"\n",
    "    return df\n",
    "\n",
    "# Apply the function to each row in the 'rewritten_text' column\n",
    "df = add_special_tokens(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "scs_model = SentenceTransformer(\"sentence-t5-base\")\n",
    "\n",
    "def  batch_cosine_similarity(x1, x2):\n",
    "\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    x1_norm = torch.nn.functional.normalize(x1, p=2, dim=-1)\n",
    "    x2_norm = torch.nn.functional.normalize(x2, p=2, dim=-1)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "def sharpened_cosine_similarity_batch(scs_model, output_texts, target_texts, sharpen_factor=3):\n",
    "    # Assuming scs_model.encode() returns PyTorch tensors\n",
    "    target_embeddings = scs_model.encode(target_texts, convert_to_tensor=True)\n",
    "    output_embeddings = scs_model.encode(output_texts, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate batch cosine similarities using the previously defined batch_cosine_similarity function\n",
    "    cos_sims = batch_cosine_similarity(target_embeddings, output_embeddings)\n",
    "    \n",
    "    # Apply the sharpening factor to each similarity score and create a list of tensors\n",
    "    sharpened_scores = [cos_sims[i][i].unsqueeze(0) ** sharpen_factor for i in range(cos_sims.size(0))]\n",
    "    \n",
    "    return sharpened_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('model/gpt2/')\n",
    "model = GPT2LMHeadModel.from_pretrained('model/gpt2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8673, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/573 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m input_text \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_text_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m----> 6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adjust the max_length parameter as needed\u001b[39;00m\n\u001b[0;32m     10\u001b[0m output_sequences \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     11\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     12\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,  \u001b[38;5;66;03m# Adjust the max length of the output text if needed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Number of output sequences to generate\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "scs = []\n",
    "CLEAN=0\n",
    "for i in tqdm(range(8100,8673)):#input_text = \"I get it now. You have to give to get. Where's did I put the cereal? You ca n't have life without death. Or love without hate. I think this milk is sour. Maybe it's fine. Maybe I'm fine, just the way I am.   I have to shower before I go. Maybe the only reason everything is so shit is because I expect it to be. A positive attitude might be the only thing I need. Shit, I'm going to be late.   Learn to drive, asshole! I need to relax, like in that dream, some real Zen stuff. Love and equality and all that. Fuck, I spilled my coffee. Today is going to be hell.<|endoftext|> The cosmic tapestry of existence unraveled before me, illuminating the interconnectedness of all phenomena. I perceive the enigmatic axiom woven through the threads of life and death, love and hate. It is an axiom inscribed in the cosmic consciousness, a testament to the duality of existence.Through the prism of perception, I discern the paradoxical dance of giving and receiving. To acquire the sustenance necessary for survival, one must offer a reciprocal gift, a harmonious exchange of energy. The cosmic rhythm dictates that to emerge unscathed, one must embrace the yin and yang of life, death, and all that encompasses them.The sourness of the milk serves as a reminder of the impermanence of all temporal endeavors. Yet, within the abyss of doubt, I find solace in the recognition of my own intrinsic wholeness. Perhaps, the paradox of existence is reconciled through a profound sense of self-<|endoftext|>\"  # This is your input text\n",
    "    input_text = df[\"combined_text_inf\"][i]\n",
    "    prompt = df[\"prompt\"][i]\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    # Adjust the max_length parameter as needed\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=1024,  # Adjust the max length of the output text if needed\n",
    "        temperature=0.1,  # Temperature controls the randomness of the output\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Nucleus sampling\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1  # Number of output sequences to generate\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "    #print(\"Generated text:\", generated_text)\n",
    "    pred_prompt = generated_text.split(\"The prompt used to generate rewritten text given orignal text is:\")[-1]\n",
    "    a=[pred_prompt]\n",
    "    b=[prompt]\n",
    "    score = sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)\n",
    "    scs.append(score[0].item())\n",
    "    CLEAN+=1\n",
    "    if CLEAN%100==0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4351041913032532"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.3568], device='cuda:0')]\n",
      "  \"\"\"\"\"\"\" Dear Diary Hero,\"\" I wrote yesterday morning through my worn laptop screen while browsing news archives at /r/fallenworldnews. As usual it took me quite some time even writing\n",
      "Turn it into a classic Greek tragedy.\n"
     ]
    }
   ],
   "source": [
    "i=8003\n",
    "\n",
    "input_text = df[\"combined_text_inf\"][i]\n",
    "prompt = df[\"prompt\"][i]\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    # Adjust the max_length parameter as needed\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=len(input_ids[0])+40,  # Adjust the max length of the output text if needed\n",
    "    temperature=1.0,  # Temperature controls the randomness of the output\n",
    "    top_k=50,  # Top-k sampling\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1  # Number of output sequences to generate\n",
    ")\n",
    "\n",
    "    # Decode the output\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "    #print(\"Generated text:\", generated_text)\n",
    "pred_prompt = generated_text.split(\"The prompt used to generate rewritten text given original text is:\")[-1]\n",
    "\n",
    "\n",
    "a=[pred_prompt]\n",
    "b=[prompt]\n",
    "score = sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)\n",
    "print(score)\n",
    "print(pred_prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Original text: Karanpura is an archeological site near Bhadra city of Hanumangarh district in Rajasthan, India. It belongs with ancient Indus Valley Civilization. Harappan pottery has been found after excavation. <|endoftext|> Rewritten text: Karanpura 🏭️ is an archeological site near Bhadra 🌃 city of Hanumangarh 🗺️ district in Rajasthan 🇮🇳, India. It belongs with ancient Indus Valley Civilization 🌍. Harappan urns pottery has been found after excavation.<|endoftext|> The prompt used to generate rewritten text given original text is:  `` Dear reader ’ s detective ( Mr Miller ), we've unearthed a cache that contains the finest preserved Roman and Medieval coins from around Ancient Rome! Excavation revealed a wealth trove depicting treasures such\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2016 Presidential Race Script\"\\n\"Original text: He was dead. He went down. He had massive trauma to his face. If he was n\\'t dead, he would be'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text.split(\"The prompt used to generate rewritten text given orignal text is:\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> Rewritten text:This is a very inspiring article I have a question about your rmse'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = \"<|endoftext|> Rewritten text:This is a very inspiring article I have a question about your rmse\"\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3861], device='cuda:0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\"1920s Gangster Slang\"]\n",
    "b=[prompt]\n",
    "sharpened_cosine_similarity_batch(scs_model, a, b, sharpen_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1) if 1==2 else print(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
